# **Day 3: PySpark Transformations Deep Dive**

---

## ğŸ” Topics Learned:

ğ—£ğ˜†ğ—¦ğ—½ğ—®ğ—¿ğ—¸ ğ˜ƒğ˜€ ğ—£ğ—®ğ—»ğ—±ğ—®ğ˜€
- Pandas works well for small, in-memory datasets, but ğ—£ğ˜†ğ—¦ğ—½ğ—®ğ—¿ğ—¸ ğ—²ğ—»ğ—®ğ—¯ğ—¹ğ—²ğ˜€ ğ—±ğ—¶ğ˜€ğ˜ğ—¿ğ—¶ğ—¯ğ˜‚ğ˜ğ—²ğ—± ğ—½ğ—¿ğ—¼ğ—°ğ—²ğ˜€ğ˜€ğ—¶ğ—»ğ—´, making it suitable for large-scale data with better performance and fault tolerance.

ğ—ğ—¼ğ—¶ğ—»ğ˜€ (ğ—œğ—»ğ—»ğ—²ğ—¿, ğ—Ÿğ—²ğ—³ğ˜, ğ—¥ğ—¶ğ—´ğ—µğ˜, ğ—¢ğ˜‚ğ˜ğ—²ğ—¿)
- Joins allow combining multiple datasets based on a common key. In PySpark, joins are optimized for lğ—®ğ—¿ğ—´ğ—² ğ—±ğ—®ğ˜ğ—® ğ˜ƒğ—¼ğ—¹ğ˜‚ğ—ºğ—²ğ˜€ and are essential for building analytical datasets from raw tables.

ğ—ªğ—¶ğ—»ğ—±ğ—¼ğ˜„ ğ—™ğ˜‚ğ—»ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ˜€ (ğ—¥ğ˜‚ğ—»ğ—»ğ—¶ğ—»ğ—´ ğ—§ğ—¼ğ˜ğ—®ğ—¹ğ˜€, ğ—¥ğ—®ğ—»ğ—¸ğ—¶ğ—»ğ—´ğ˜€)
- Window functions help calculate metrics like cumulative counts, running totals, and rankings without reducing the number of rows â€” extremely useful for ğ˜‚ğ˜€ğ—²ğ—¿ ğ—¯ğ—²ğ—µğ—®ğ˜ƒğ—¶ğ—¼ğ—¿ ğ—®ğ—»ğ—± ğ˜ğ—¶ğ—ºğ—²-ğ—¯ğ—®ğ˜€ğ—²ğ—± ğ—®ğ—»ğ—®ğ—¹ğ˜†ğ˜€ğ—¶ğ˜€.

ğ—¨ğ˜€ğ—²ğ—¿-ğ——ğ—²ğ—³ğ—¶ğ—»ğ—²ğ—± ğ—™ğ˜‚ğ—»ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ˜€ (ğ—¨ğ——ğ—™ğ˜€)
- UDFs allow applying custom business logic when built-in Spark functions are not sufficient, helping in feature engineering, though they should be used carefully due to performance impact.

---

### ğŸ› ï¸ Hands-on Tasks Completed (Databricks Notebook):

- Loaded the full e-commerce dataset
- Performed complex joins across datasets
- Calculated running totals using window functions
- Created derived features for deeper insights

--- 

###  ğŸ“š Learning References & Acknowledgements:
This learning initiative is supported by the data community and learning resources from:
