# **DAY 1 â€“ Platform Setup & First Steps**

## ğŸ“˜ Things Learnt:
- Why Databricks vs Pandas/Hadoop?
- Lakehouse architecture basics
- Databricks workspace structure
- Industry use cases (Netflix, Shell, Comcast)

## ğŸ› ï¸ Tasks Completed:
1. Create Databricks Community Edition account
2. Navigate Workspace, Compute, Data Explorer
3. Create first notebook
4. Run basic PySpark commands

## ğŸ“Š Practice with Data:

### Create simple DataFrame:

data = [("iPhone", 999), ("Samsung", 799), ("MacBook", 1299)]

df = spark.createDataFrame(data, ["product", "price"])

df.show()

| Product | Price |
|--------|-------|
| iPhone | 999 |
| Samsung | 799 |
| MacBook | 1299 |



### Filter expensive products:

df.filter(df.price > 1000).show()

|product|price|
|-------|-----|
|MacBook| 1299|



### ğŸ”‘ Key Takeaways:
- Databricks provides an integrated **Lakehouse platform** combining data engineering and analytics.
- PySpark enables **scalable data transformations** using lazy evaluation.
- Databricks notebooks offer an **interactive and efficient development environment**.

---

### ğŸ› ï¸ Tools & Technologies:
- Databricks Community Edition
- PySpark

---

### ğŸ“š Learning References & Acknowledgements:
This learning initiative is supported by the data community and learning resources from:
- **Databricks** â€“ https://www.databricks.com
- **Codebasics** â€“ https://www.codebasics.io
- **Indian Data Club** â€“ https://indiandataclub.com


### ğŸ”– Tags & Mentions:
#Databricks #Codebasics #IndianDataClub #DatabricksWithIDC









