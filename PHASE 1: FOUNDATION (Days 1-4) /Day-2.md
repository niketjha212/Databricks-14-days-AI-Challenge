# ğ——ğ—®ğ˜†-ğŸ®: Apache Spark Fundamentals

---

## Things Learned:
- Spark Architecture: Understood the roles of Driver, Executors, and how Spark uses DAG (Directed Acyclic Graph) to optimize execution.
- DataFrames vs RDDs : Learned why DataFrames are preferred over RDDs due to optimization, ease of use, and Catalyst optimizer.
- Lazy Evaluation : Spark doesnâ€™t execute immediately â€” actions trigger execution, helping Spark optimize the entire workflow efficiently.
- Notebook Magic Commands :Got hands-on with Databricks magic commands like: %sql, %python, %fs for seamless multi-language execution.
- Importance of col() â€“ Used to refer to columns in PySpark for transformations, filtering, null checks, and calculations

--- 

## Tasks Completed:
- Uploaded a sample e-commerce CSV file.
- Created a DataFrame using PySpark
- Performed basic operations : select(), show(), dtypes(), filter(), groupBy(), orderBy()
- Used col() to handle columns safely and perform null checks.

--- 

## Tags:
@Databricks @codebasics @ndiandataclub   
